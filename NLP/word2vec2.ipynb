{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n",
    "Learning can be supervised, semi-supervised or unsupervised.\n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "Artificial neural networks  were inspired by information processing and distributed communication nodes in biological systems. \n",
    "ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, \n",
    "while the biological brain of most living organisms is dynamic (plastic) and analog.\n",
    "The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. \n",
    "Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\n",
    "Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions.\n",
    "In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, hence the \"structured\" part.\n",
    "Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\n",
    "The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.\n",
    "In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.\n",
    "Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.\n",
    "The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. \n",
    "Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension,\n",
    "then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\n",
    "The probabilistic interpretation derives from the field of machine learning. \n",
    "It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. \n",
    "More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.\n",
    "The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. \n",
    "The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word2vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "text_data = preprocess(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "def create_tokens(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = create_tokens(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishabhsharma/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Stopwords\n",
    "def remove_stopwords(word_tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [[w for w in word if not w in stop_words] for word in word_tokens]\n",
    "\n",
    "word_tokens = remove_stopwords(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "def lemmatize_words(filtered_tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [[lemmatizer.lemmatize(w) for w in word] for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_output = lemmatize_words(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2Vec Model\n",
    "model = Word2Vec(lemmatized_output, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('considers', 0.21648749709129333), ('capacity', 0.18907596170902252), ('one', 0.1884191483259201), ('science', 0.1849723756313324), ('living', 0.18418720364570618), ('allowed', 0.18141457438468933), ('produced', 0.18017159402370453), ('widely', 0.17434096336364746), ('trainability', 0.1719604879617691), ('survey', 0.1692296266555786)]\n"
     ]
    }
   ],
   "source": [
    "## Similarity\n",
    "sim_words = model.wv.most_similar('deep')\n",
    "print(sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Word2Vec Model\n",
    "model = Word2Vec(lemmatized_output, min_count=1, window = 5, sg = 1)\n",
    "\n",
    "## Similarity\n",
    "sim_words = model.wv.most_similar('deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('considers', 0.2331845611333847),\n",
       " ('capacity', 0.19747082889080048),\n",
       " ('produced', 0.19468210637569427),\n",
       " ('one', 0.19266347587108612),\n",
       " ('science', 0.19240933656692505),\n",
       " ('allowed', 0.19074752926826477),\n",
       " ('widely', 0.18849675357341766),\n",
       " ('living', 0.18813900649547577),\n",
       " ('trainability', 0.1873253434896469),\n",
       " ('analysis', 0.17168308794498444)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('interpretation', 0.3232540786266327), ('translation', 0.2383796125650406), ('work', 0.23218710720539093), ('distribution', 0.2225135862827301), ('led', 0.20174400508403778), ('machine', 0.19200050830841064), ('deep', 0.18813902139663696), ('result', 0.1761387288570404), ('hence', 0.17364022135734558), ('comparable', 0.17268885672092438)]\n"
     ]
    }
   ],
   "source": [
    "sim_words = model.wv.most_similar('living')\n",
    "print(sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
