{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokenized):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    for i, word in enumerate(tokenized):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, id2word = mapping(tokenized=tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def one_hot(word2id, id2word):\n",
    "    vocab_size = len(word2id)\n",
    "    one_hot = np.zeros((vocab_size, vocab_size))\n",
    "    for i in range(vocab_size):\n",
    "        one_hot[i, i] = 1\n",
    "    return one_hot\n",
    "\n",
    "one_hot = one_hot(word2id=word2id, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def window(tokenized, window_size):\n",
    "    for i, word in enumerate(tokenized):\n",
    "        for j in range(1, window_size+1):\n",
    "            if i-j >= 0:\n",
    "                yield word, tokenized[i-j]\n",
    "            if i+j < len(tokenized):\n",
    "                yield word, tokenized[i+j]\n",
    "\n",
    "window = window(tokenized=tokenized, window_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(window, word2id, id2word, one_hot, learning_rate=0.01, epochs=1000):\n",
    "    vocab_size = len(word2id)\n",
    "    weights_0_1 = np.random.rand(vocab_size, vocab_size) - 0.5\n",
    "    weights_1_2 = np.random.rand(vocab_size, vocab_size) - 0.5\n",
    "    for epoch in range(epochs):\n",
    "        for word, context in window:\n",
    "            x = one_hot[word2id[word]]\n",
    "            y = one_hot[word2id[context]]\n",
    "            layer_1 = np.dot(weights_0_1, x)\n",
    "            layer_2 = np.dot(weights_1_2, layer_1)\n",
    "            error = layer_2 - y\n",
    "            layer_2_delta = error\n",
    "            layer_1_delta = np.dot(weights_1_2.T, layer_2_delta)\n",
    "            weights_1_2 -= learning_rate * np.outer(layer_2_delta, layer_1)\n",
    "            weights_0_1 -= learning_rate * np.outer(layer_1_delta, x)\n",
    "    return weights_0_1, weights_1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "w1, w2 = word2vec(window=window, word2id=word2id, id2word=id2word, one_hot=one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def similar_words(word, word2id, id2word, w1, w2):\n",
    "    word_id = word2id[word]\n",
    "    word_vector = w1[word_id]\n",
    "    scores = np.dot(w2, word_vector)\n",
    "    scores = softmax(scores)\n",
    "    scores = list(zip(scores, id2word.values()))\n",
    "    scores.sort(reverse=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06285710819784264, 'intelligence'),\n",
       " (0.04786534684039764, 'build'),\n",
       " (0.04573188767027622, 'do'),\n",
       " (0.03996979111798652, 'are'),\n",
       " (0.038588823005043155, 'a')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "similar_words(word='machine', word2id=word2id, id2word=id2word, w1=w1, w2=w2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13468326, -0.19684243,  0.43857472, -0.28476414,  0.43530193,\n",
       "        0.20447942,  0.25002282,  0.30948924,  0.0208748 ,  0.10558158,\n",
       "       -0.13203783, -0.02213319,  0.11203683,  0.40720176, -0.21561734,\n",
       "        0.29040601, -0.26544885, -0.33283   ,  0.14124175,  0.08274958,\n",
       "        0.29810613, -0.34943502,  0.46019817,  0.40337427,  0.05323945,\n",
       "        0.1453813 ,  0.36344768,  0.32756699, -0.37171059,  0.29042301,\n",
       "       -0.25503605,  0.13323524,  0.20662686,  0.15085281, -0.21277151,\n",
       "        0.48674378,  0.37128361,  0.05037762, -0.46280021, -0.25772193,\n",
       "        0.17332507,  0.16785123,  0.36351297, -0.14005013, -0.48910117,\n",
       "       -0.49207818,  0.28811948, -0.06668405, -0.05826786, -0.07328144,\n",
       "       -0.09096476, -0.39326109,  0.23098222, -0.39833556,  0.26990071,\n",
       "        0.25530312, -0.10826231, -0.32549625, -0.18167538,  0.48704778])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[word2id['machine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29216858,  0.47645538, -0.19693512, -0.01367317,  0.24567181,\n",
       "        0.42751174,  0.09396516, -0.25403183, -0.38774624, -0.17775924,\n",
       "        0.32292971,  0.20740634,  0.38552175, -0.04574454, -0.40140449,\n",
       "        0.18973747,  0.43054018,  0.28205653,  0.24641479, -0.39899754,\n",
       "        0.29038271,  0.22560332,  0.1574376 ,  0.22181724, -0.4854422 ,\n",
       "       -0.25787095, -0.01659284, -0.02262975,  0.0963914 , -0.39273031,\n",
       "        0.14829205, -0.17104214, -0.49802357,  0.34730546,  0.24221292,\n",
       "        0.14039679,  0.1809301 , -0.02383346, -0.00268695, -0.02623146,\n",
       "        0.35729833,  0.16347112,  0.18387644, -0.29997197, -0.33602763,\n",
       "        0.17678612,  0.26208991,  0.04608929, -0.03542888,  0.21480819,\n",
       "        0.22672015,  0.11188807, -0.43929241, -0.48027932, -0.07656317,\n",
       "        0.35428917, -0.38303344,  0.4661639 , -0.32523375, -0.25215594])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[word2id['learning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25810638, -0.13163294,  0.16932508, -0.49139265,  0.38721403,\n",
       "        0.35591421, -0.06708646, -0.11569176, -0.09385104,  0.26142946,\n",
       "       -0.36942467,  0.01446076,  0.22392839,  0.40771571,  0.4036527 ,\n",
       "        0.10397787, -0.20667134, -0.22792709,  0.37990906,  0.11773941,\n",
       "       -0.17916121,  0.40423394,  0.33439644,  0.0058892 , -0.03760155,\n",
       "       -0.35430086,  0.11343007,  0.33909895,  0.42559444,  0.3420938 ,\n",
       "        0.36898447, -0.13713464, -0.15831006,  0.18651214,  0.2235192 ,\n",
       "        0.47448655, -0.49198456,  0.33806272,  0.30222539, -0.18782532,\n",
       "        0.47874395,  0.19001745,  0.07843145, -0.17508503,  0.25764148,\n",
       "        0.30170378,  0.08040033,  0.34119958,  0.06547649,  0.385133  ,\n",
       "        0.23335953,  0.27634001,  0.49539856,  0.21438253, -0.14357298,\n",
       "        0.29809227,  0.22003126,  0.17160774, -0.43936182, -0.33146934])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[word2id['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
