{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokenized):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    for i, word in enumerate(tokenized):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, id2word = mapping(tokenized=tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def one_hot(word2id, id2word):\n",
    "    vocab_size = len(word2id)\n",
    "    one_hot = np.zeros((vocab_size, vocab_size))\n",
    "    for i in range(vocab_size):\n",
    "        one_hot[i, i] = 1\n",
    "    return one_hot\n",
    "\n",
    "one_hot = one_hot(word2id=word2id, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def window(tokenized, window_size):\n",
    "    for i, word in enumerate(tokenized):\n",
    "        for j in range(1, window_size+1):\n",
    "            if i-j >= 0:\n",
    "                yield word, tokenized[i-j]\n",
    "            if i+j < len(tokenized):\n",
    "                yield word, tokenized[i+j]\n",
    "\n",
    "window = window(tokenized=tokenized, window_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(window, word2id, id2word, one_hot, learning_rate=0.01, epochs=1000):\n",
    "    vocab_size = len(word2id)\n",
    "    weights_0_1 = np.random.rand(vocab_size, vocab_size) - 0.5\n",
    "    weights_1_2 = np.random.rand(vocab_size, vocab_size) - 0.5\n",
    "    for epoch in range(epochs):\n",
    "        for word, context in window:\n",
    "            x = one_hot[word2id[word]]\n",
    "            y = one_hot[word2id[context]]\n",
    "            layer_1 = np.dot(weights_0_1, x)\n",
    "            layer_2 = np.dot(weights_1_2, layer_1)\n",
    "            error = layer_2 - y\n",
    "            layer_2_delta = error\n",
    "            layer_1_delta = np.dot(weights_1_2.T, layer_2_delta)\n",
    "            weights_1_2 -= learning_rate * np.outer(layer_2_delta, layer_1)\n",
    "            weights_0_1 -= learning_rate * np.outer(layer_1_delta, x)\n",
    "    return weights_0_1, weights_1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "\n",
    "w1, w2 = word2vec(window=window, word2id=word2id, id2word=id2word, one_hot=one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def similar_words(word, word2id, id2word, w1, w2):\n",
    "    word_id = word2id[word]\n",
    "    word_vector = w1[word_id]\n",
    "    scores = np.dot(w2, word_vector)\n",
    "    scores = softmax(scores)\n",
    "    scores = list(zip(scores, id2word.values()))\n",
    "    scores.sort(reverse=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06285710819784264, 'intelligence'),\n",
       " (0.04786534684039764, 'build'),\n",
       " (0.04573188767027622, 'do'),\n",
       " (0.03996979111798652, 'are'),\n",
       " (0.038588823005043155, 'a')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "similar_words(word='machine', word2id=word2id, id2word=id2word, w1=w1, w2=w2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
